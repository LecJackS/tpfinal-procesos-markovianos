{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* DONE :) Fijate si podes imprimir el espacio de Q a medida que avanza en el training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T02:58:23.393548Z",
     "start_time": "2019-06-24T02:58:23.385803Z"
    }
   },
   "outputs": [],
   "source": [
    "# qlearningAgents.py\n",
    "# ------------------\n",
    "# Licensing Information:  You are free to use or extend these projects for\n",
    "# educational purposes provided that (1) you do not distribute or publish\n",
    "# solutions, (2) you retain this notice, and (3) you provide clear\n",
    "# attribution to UC Berkeley, including a link to http://ai.berkeley.edu.\n",
    "# \n",
    "# Attribution Information: The Pacman AI projects were developed at UC Berkeley.\n",
    "# The core projects and autograders were primarily created by John DeNero\n",
    "# (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n",
    "# Student side autograding was added by Brad Miller, Nick Hay, and\n",
    "# Pieter Abbeel (pabbeel@cs.berkeley.edu).\n",
    "\n",
    "# for importing notebooks (.ipynb) as regular .py\n",
    "import import_ipynb\n",
    "from game import *\n",
    "from learningAgents import ReinforcementAgent\n",
    "from featureExtractors import *\n",
    "\n",
    "import random,util,math\n",
    "import numpy as np\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T02:58:23.398655Z",
     "start_time": "2019-06-24T02:58:23.395539Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_argmax(v):\n",
    "    \"\"\"Like np.argmax(), but if there are several \"best\" actions,\n",
    "       chooses and returns one randomly\"\"\"\n",
    "    arguments = np.argwhere(v == np.amax(v)).ravel()\n",
    "    return np.random.choice(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T15:41:33.958403Z",
     "start_time": "2019-06-24T15:41:33.793875Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7f2f6f7d7fd0>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T02:58:23.415212Z",
     "start_time": "2019-06-24T02:58:23.401672Z"
    },
    "code_folding": [
     2
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# From layout of chars to layout of numbers\n",
    "# Beware: This will be painful to see\n",
    "def ascii_state_to_numeric_state(ascii_state):\n",
    "    str_state = str(ascii_state)\n",
    "    score_pos = str(str_state).find(\"Score: \")\n",
    "    ascii_map = str(str_state)[:score_pos-1]\n",
    "\n",
    "    numer_map = np.ndarray(len(ascii_map)+1, dtype=np.double)\n",
    "    for i, c in enumerate(ascii_map):\n",
    "        if c==' ':\n",
    "            numer_map[i] = 1\n",
    "            continue\n",
    "        if c=='%':\n",
    "            numer_map[i] = 2\n",
    "            continue\n",
    "        if c=='.':\n",
    "            numer_map[i] = 3\n",
    "            continue\n",
    "        if c=='\\n':\n",
    "            numer_map[i] = 4\n",
    "            continue\n",
    "        if c=='G':\n",
    "            numer_map[i] = 5\n",
    "            continue\n",
    "        if c=='o':\n",
    "            numer_map[i] = 6\n",
    "            continue\n",
    "        # Pacman dirs\n",
    "        if c=='<':\n",
    "            numer_map[i] = 7\n",
    "            continue\n",
    "        if c=='>':\n",
    "            numer_map[i] = 8\n",
    "            continue\n",
    "        if c=='^':\n",
    "            numer_map[i] = 9\n",
    "            continue\n",
    "        if c=='v':\n",
    "            numer_map[i] = 10\n",
    "            continue\n",
    "    numer_map /= 15.0\n",
    "    #last array position will contain the score\n",
    "    numer_map[-1] = float(str_state[score_pos+7:])/3000\n",
    "    return numer_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T15:42:20.947840Z",
     "start_time": "2019-06-24T15:42:20.920678Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# From layout of chars to layout of one hot vectors\n",
    "# Beware: This will be painful to see\n",
    "def ascii_state_to_one_hots_state(ascii_state):\n",
    "    str_state = str(ascii_state)\n",
    "    score_pos = str(str_state).find(\"Score: \")\n",
    "    ascii_map = str(str_state)[:score_pos-1]\n",
    "    \n",
    "    total_symbols = 10+1 # one extra for unknown ascii\n",
    "    total_elements = len(ascii_map)\n",
    "    one_hots_map = np.ndarray((total_elements, total_symbols), dtype=np.double)\n",
    "    one_hots_map.fill(0)\n",
    "    for i, c in enumerate(ascii_map):\n",
    "        if c==' ':\n",
    "            one_hots_map[i,0] = 1.\n",
    "            continue\n",
    "        if c=='%':\n",
    "            one_hots_map[i,1] = 1.\n",
    "            continue\n",
    "        if c=='.':\n",
    "            one_hots_map[i,2] = 1.\n",
    "            continue\n",
    "        if c=='\\n':\n",
    "            one_hots_map[i,3] = 1.\n",
    "            continue\n",
    "        if c=='G':\n",
    "            one_hots_map[i,4] = 1.\n",
    "            continue\n",
    "        if c=='o':\n",
    "            one_hots_map[i,5] = 1.\n",
    "            continue\n",
    "        # Pacman directions\n",
    "        if c=='<':\n",
    "            one_hots_map[i,6] = 1.\n",
    "            continue\n",
    "        if c=='>':\n",
    "            one_hots_map[i,7] = 1.\n",
    "            continue\n",
    "        if c=='^':\n",
    "            one_hots_map[i,8] = 1.\n",
    "            continue\n",
    "        if c=='v':\n",
    "            one_hots_map[i,9] = 1.\n",
    "            continue\n",
    "        else:\n",
    "            # Unknown symbol\n",
    "            print(\"Beware! Unknown symbol one-hotted from layout: \", c)\n",
    "            one_hots_map[i,10] = 1.\n",
    "    #last array position will contain the score\n",
    "    score = float(str_state[score_pos+7:])\n",
    "    # matrix to 1D array ++ score/3000 as some normalization\n",
    "    input_data = np.concatenate((one_hots_map.reshape(-1), [score/3000.]))\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Q Learning\n",
    "\n",
    "Q(s,a) is a dictionary with each state-action value it visits  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T02:58:23.442270Z",
     "start_time": "2019-06-24T02:58:23.417441Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "class QLearningAgent(ReinforcementAgent):\n",
    "    # Parent class in learningAgents.py\n",
    "    \"\"\" Q-Learning Agent\n",
    "\n",
    "      Functions you should fill in:\n",
    "        - computeValueFromQValues\n",
    "        - computeActionFromQValues\n",
    "        - getQValue\n",
    "        - getAction\n",
    "        - update\n",
    "\n",
    "      Instance variables you have access to\n",
    "        - self.epsilon (exploration prob)\n",
    "        - self.alpha (learning rate)\n",
    "        - self.discount (discount rate)\n",
    "\n",
    "      Functions you should use\n",
    "        - self.getLegalActions(state)\n",
    "          which returns legal actions for a state\n",
    "    \"\"\"\n",
    "    def __init__(self, **args):\n",
    "        \"You can initialize Q-values here...\"\n",
    "        ReinforcementAgent.__init__(self, **args)\n",
    "        \n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        #self.Q = {}\n",
    "        self.Q = Counter()\n",
    "        #self.q_size_history = np.ndarray((self.numTraining//10+1, 2))\n",
    "\n",
    "    def getQValue(self, state, action):\n",
    "        \"\"\"\n",
    "          Returns Q(state,action)\n",
    "          Should return 0.0 if we have never seen a state\n",
    "          or the Q node value otherwise\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        return self.Q[(state, action)]\n",
    "\n",
    "\n",
    "    def computeValueFromQValues(self, state):\n",
    "        \"\"\"\n",
    "          Returns max_action Q(state,action)\n",
    "          where the max is over legal actions.  Note that if\n",
    "          there are no legal actions, which is the case at the\n",
    "          terminal state, you should return a value of 0.0.\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # max_a(Q[state, all actions])\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            value=0.0\n",
    "        else:\n",
    "            # TODO: Find a better way\n",
    "            value=max([self.getQValue(state, a) for a in legalActions])\n",
    "        return value\n",
    "\n",
    "    def computeActionFromQValues(self, state):\n",
    "        \"\"\"\n",
    "          Compute the best action to take in a state.  Note that if there\n",
    "          are no legal actions, which is the case at the terminal state,\n",
    "          you should return None.\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            action=None\n",
    "        else:\n",
    "            # TODO: Find a better way\n",
    "            action=legalActions[random_argmax([self.getQValue(state, a) for a in legalActions])]\n",
    "        return action\n",
    "\n",
    "    def getAction(self, state):\n",
    "        \"\"\"\n",
    "          Compute the action to take in the current state.  With\n",
    "          probability self.epsilon, we should take a random action and\n",
    "          take the best policy action otherwise.  Note that if there are\n",
    "          no legal actions, which is the case at the terminal state, you\n",
    "          should choose None as the action.\n",
    "        \"\"\"\n",
    "        # Pick Action\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            #action = None\n",
    "            return None\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # epsilon decay\n",
    "        epsmin = 0.01\n",
    "        eps_decay = 0.9999\n",
    "        self.epsilon = max(self.epsilon*eps_decay, epsmin)\n",
    "        if util.flipCoin(self.epsilon):\n",
    "            # Act randomly\n",
    "            action = random.choice(legalActions)\n",
    "        else:\n",
    "            # Act greedly\n",
    "            action = self.computeActionFromQValues(state)\n",
    "        return action\n",
    "        \n",
    "    def update(self, state, action, nextState, reward, terminal_state=False):\n",
    "        \"\"\"\n",
    "          The parent class calls this to observe a\n",
    "          state = action => nextState and reward transition.\n",
    "          You should do your Q-Value update here\n",
    "\n",
    "          NOTE: You should never call this function,\n",
    "          it will be called on your behalf\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        iteration = self.episodesSoFar\n",
    "        self.alpha = 1/np.power((iteration+1), 1) # alpha decay\n",
    "        alpha = self.alpha\n",
    "        gamma = self.discount\n",
    "        if not terminal_state:\n",
    "            advantage = reward + gamma*self.computeValueFromQValues(nextState) - self.Q[(state, action)]\n",
    "        else:\n",
    "            advantage = reward - self.Q[(state, action)]\n",
    "        self.Q[(state, action)] += alpha * advantage\n",
    "\n",
    "    def getPolicy(self, state):\n",
    "        return self.computeActionFromQValues(state)\n",
    "\n",
    "    def getValue(self, state):\n",
    "        return self.computeValueFromQValues(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T02:58:23.448500Z",
     "start_time": "2019-06-24T02:58:23.443910Z"
    },
    "cell_style": "center",
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class PacmanQAgent(QLearningAgent):\n",
    "    \"Exactly the same as QLearningAgent, but with different default parameters\"\n",
    "\n",
    "    def __init__(self, epsilon=0.05,gamma=0.8,alpha=0.2, numTraining=0, **args):\n",
    "        \"\"\"\n",
    "        These default parameters can be changed from the pacman.py command line.\n",
    "        For example, to change the exploration rate, try:\n",
    "            python pacman.py -p PacmanQLearningAgent -a epsilon=0.1\n",
    "\n",
    "        alpha    - learning rate/step size\n",
    "        epsilon  - exploration rate\n",
    "        gamma    - discount factor\n",
    "        numTraining - number of training episodes, i.e. no learning after these many episodes\n",
    "        \"\"\"\n",
    "        args['epsilon'] = epsilon\n",
    "        args['gamma'] = gamma\n",
    "        args['alpha'] = alpha\n",
    "        args['numTraining'] = numTraining\n",
    "        self.index = 0  # This is always Pacman\n",
    "        QLearningAgent.__init__(self, **args)\n",
    "\n",
    "    def getAction(self, state):\n",
    "        \"\"\"\n",
    "        Simply calls the getAction method of QLearningAgent and then\n",
    "        informs parent of action for Pacman.  Do not change or remove this\n",
    "        method.\n",
    "        \"\"\"\n",
    "        action = QLearningAgent.getAction(self,state)\n",
    "        self.doAction(state,action)\n",
    "        #print(\"Q (table) size: \"+str(len(self.Q)), end=\"\\r\")\n",
    "        return action\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PacmanRandomAgent(QLearningAgent):\n",
    "    \"Random agent: Chooses random action at each time step\"\n",
    "    def __init__(self, epsilon=0.05, gamma=0.8, alpha=0.2, numTraining=0, **args):\n",
    "        self.index = 0  # This is always Pacman\n",
    "        QLearningAgent.__init__(self, **args)\n",
    "        del self.Q\n",
    "        \n",
    "    def update(self, state, action, nextState, reward, terminal_state=False):\n",
    "        pass\n",
    "    \n",
    "    def getAction(self, state):\n",
    "        \"\"\"\n",
    "        Simply calls the getAction method of QLearningAgent and then\n",
    "        informs parent of action for Pacman.  Do not change or remove this\n",
    "        method.\n",
    "        \"\"\"\n",
    "        # Pick Action\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            return None\n",
    "        \n",
    "        action = random.choice(legalActions)\n",
    "        self.doAction(state, action)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Aproximation for Q learning\n",
    "\n",
    "Sutton and Barto Ch.9.5 - p.232 *\"Feature construction for linear methods\"*\n",
    "\n",
    "We want to generalize Q values better for different state-action pairs.\n",
    "\n",
    "Intuition:\n",
    "\n",
    "* If a ghost is close to pacman at one state and dies, we want to generalize \"danger\" to any other position where a ghost is close.\n",
    "\n",
    "\n",
    "Observations:\n",
    "\n",
    "* Linear approximators **can't** find relationships between features, so we need to combine them ourselves (if we want that)\n",
    " \n",
    " eg:\n",
    " feature \"dist_x\" represents horizontal distance to ghost,\n",
    " feature \"dist_y\" represents vertical distance to ghost,\n",
    " \n",
    " A linear approximator cannot learn if a ghost is close on a plane, because it cannot make operations inbetween features to get a combined value.\n",
    " \n",
    " To solve this, we can add a third feature that combines the other two:\n",
    " \n",
    " *feature\\[ \"dist_xy\" \\] = dist_x $*$ dist_x*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n $ : number of features\n",
    "\n",
    "$Q(s,a) = \\sum\\limits_{i=1}^n f_i(s,a) * w_i$\n",
    "\n",
    "**Prediction error:**\n",
    "\n",
    "$advantage = (R + \\gamma \\max\\limits_{a} Q(S', a)) - Q(S,A)$\n",
    "\n",
    "**Update:**\n",
    "\n",
    "$w_i \\leftarrow w_i + \\alpha \\cdot advantage \\cdot f_i(S,A)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T02:58:23.460188Z",
     "start_time": "2019-06-24T02:58:23.450143Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class ApproximateQAgent(PacmanQAgent):\n",
    "    \"\"\"\n",
    "       ApproximateQLearningAgent\n",
    "\n",
    "       You should only have to overwrite getQValue\n",
    "       and update.  All other QLearningAgent functions\n",
    "       should work as is.\n",
    "    \"\"\"\n",
    "    def __init__(self, extractor='IdentityExtractor', **args):\n",
    "        #extractor = 'CoordinateExtractor'\n",
    "        self.featExtractor = util.lookup(extractor, globals())()\n",
    "        PacmanQAgent.__init__(self, **args)\n",
    "        del self.Q # Not needed anymore\n",
    "        self.weights = util.Counter()\n",
    "\n",
    "    def getWeights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def getQValue(self, state, action):\n",
    "        \"\"\"\n",
    "          Should return Q(state,action) = w * featureVector\n",
    "          where * is the dotProduct operator\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        \n",
    "        featureDict = self.featExtractor.getFeatures(state, action)\n",
    "#         for feat in featureDict.keys():\n",
    "#             self.weights[feat]*featureDict[feat]\n",
    "        #print(\"aprox Q value: \", np.dot(self.weights, featureDict))\n",
    "        return np.dot(self.weights, featureDict)\n",
    "    \n",
    "    def getMaxQValue(self, state):\n",
    "        \"\"\"\n",
    "          Returns max_action Q(state,action)\n",
    "          where the max is over legal actions.  Note that if\n",
    "          there are no legal actions, which is the case at the\n",
    "          terminal state, you should return a value of 0.0.\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # max_a(Q[state, all actions])\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            value=0.0\n",
    "        else:\n",
    "            # TODO: Find a better way\n",
    "            value=max([self.getQValue(state, a) for a in legalActions])\n",
    "        return value\n",
    "    \n",
    "    def update(self, state, action, nextState, reward, terminal_state=False):\n",
    "        \"\"\"\n",
    "           Should update your weights based on transition\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        iteration = self.episodesSoFar\n",
    "        self.alpha = 1/np.power((iteration+1), 1) # alpha decay\n",
    "        alpha = self.alpha\n",
    "        gamma = self.discount\n",
    "        #state = str(state)\n",
    "        featureDict = self.featExtractor.getFeatures(state, action)\n",
    "        #for key,feat in \n",
    "        \n",
    "        pastVal = self.getQValue(state, action)\n",
    "        if not terminal_state:\n",
    "            advantage = reward + gamma*self.getMaxQValue(nextState) - pastVal\n",
    "        else:\n",
    "            # Last call of each episode (after done)\n",
    "            advantage = reward - pastVal\n",
    "        for feature in featureDict.keys():\n",
    "            #print(\"state: \", state, \" action: \", action)\n",
    "            self.weights[feature] += alpha * advantage * featureDict[feature]\n",
    "\n",
    "    def final(self, state):\n",
    "        \"Called at the end of each game.\"\n",
    "        # call the super-class final method\n",
    "        PacmanQAgent.final(self, state)\n",
    "\n",
    "        # did we finish training?\n",
    "        if self.episodesSoFar == self.numTraining:\n",
    "            # you might want to print your weights here for debugging\n",
    "            \"*** YOUR CODE HERE ***\"\n",
    "            \n",
    "#             print(\"Weights:\")\n",
    "#             pprint.pprint(self.weights)\n",
    "#             print(\"Features:\")\n",
    "#             for k in self.weights.keys():\n",
    "#                 state, action = k\n",
    "#                 pprint.pprint(self.featExtractor.getFeatures(state, action))\n",
    "\n",
    "#             print(len(self.weights))\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "\n",
    "* Linear model with experience replay 1:18 en lecture 5RL de Hado\n",
    "* Linear model with LSTD for solving instantly best parameter for that history\n",
    "* Non-linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTD (still empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/lstd-pseudocode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For number of features: 100s-1000s\n",
    "\n",
    "For millones of features it becomes unwieldy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T02:58:23.464842Z",
     "start_time": "2019-06-24T02:58:23.461754Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# make it work\n",
    "# add forgetting\n",
    "class LSTDAgent(PacmanQAgent):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodic Semi-gradient Sarsa for Estimating Q*(s,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T22:54:22.023385Z",
     "start_time": "2019-06-20T22:54:21.747231Z"
    }
   },
   "source": [
    "![](../img/episodic-semi-gradient-sarsa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO / TOREMEMBER\n",
    "* Con el aproximador lineal tenia\n",
    "\n",
    " Q(s,a) = $\\sum$ feature(s,a)*wi\n",
    "\n",
    "\n",
    "* Acá voy a tener una NN que tiene :\n",
    "\n",
    "\n",
    "1. de entrada 2 valores: state-action pair\n",
    "2. hidden layers no se\n",
    "3. output layer cantidad de features\n",
    "\n",
    "\n",
    "* Ej: \n",
    "\n",
    "\n",
    "1. Voy a necesitar predecir valores a partir de valores de entrada\n",
    "2. Voy a necesitar los gradientes para pesar la Advantage Function\n",
    "3. Voy a necesitar actualizar los pesos de mi red neuronal <- tal vez lo pueda definir en la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T01:30:12.844706Z",
     "start_time": "2019-06-27T01:30:12.829025Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc3): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import backward, Variable\n",
    "#dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.fc1 = nn.Conv2d(in_channels=10, out_channels=6, kernel_size=(2, 1))\n",
    "        self.fc2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(2, 1))\n",
    "        # an affine operation: y = Wx + b\n",
    "        # input: 147 chars from state and 1 from action taken\n",
    "        #self.fc1 = nn.Linear(148, 100)  # \n",
    "        #self.fc1 = nn.Linear(57, 100)  # \n",
    "#         self.fc1 = nn.Linear(56, 100)  #\n",
    "#         self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(480, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        # print(x)\n",
    "        x_0 = F.relu(self.fc1(x))\n",
    "        x = F.max_pool2d(x_0, (1, 1))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.fc2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = nn.Hardtanh(min_val=-1000., max_val=1000.)(self.fc3(x))\n",
    "        \n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = nn.Hardtanh(min_val=-1000., max_val=1000.)(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "ej_net = Net()\n",
    "print(ej_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T02:58:23.499080Z",
     "start_time": "2019-06-24T02:58:23.475195Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class NNQAgentOneActionIn(PacmanQAgent):\n",
    "    \"\"\"\n",
    "       ApproximateQLearningAgent\n",
    "\n",
    "       You should only have to overwrite getQValue\n",
    "       and update.  All other QLearningAgent functions\n",
    "       should work as is.\n",
    "    \"\"\"\n",
    "    def __init__(self, extractor='IdentityExtractor', **args):\n",
    "        #extractor = 'CoordinateExtractor'\n",
    "        #self.featExtractor = util.lookup(extractor, globals())()\n",
    "        PacmanQAgent.__init__(self, **args)\n",
    "        #self.weights = util.Counter()\n",
    "        self.net = self.initNN()\n",
    "        #self.net = self.net.to('cuda:0')\n",
    "        # to float; test with double later\n",
    "        #self.net = self.net.float()\n",
    "        del self.Q\n",
    "        \n",
    "        \n",
    "    def initNN(self):\n",
    "        net = Net()\n",
    "#         torch.nn.init.xavier_uniform(net.fc1.weight.data)\n",
    "#         torch.nn.init.xavier_uniform(net.fc2.weight.data)\n",
    "#         torch.nn.init.xavier_uniform(net.fc3.weight.data)\n",
    "        torch.nn.init.uniform_(net.fc1.weight.data, 0.0, 0.001)\n",
    "        torch.nn.init.uniform_(net.fc2.weight.data, 0.0, 0.001)\n",
    "        torch.nn.init.uniform_(net.fc3.weight.data, 0.0, 0.001)\n",
    "        torch.nn.init.uniform_(net.fc1.bias.data,   0.0, 0.001)\n",
    "        torch.nn.init.uniform_(net.fc2.bias.data,   0.0, 0.001)\n",
    "        torch.nn.init.uniform_(net.fc3.bias.data,   0.0, 0.001)\n",
    "        #torch.nn.init.xavier_uniform(net.weight)\n",
    "        #net.bias.data.fill_(0.01)\n",
    "        # Create random Tensors for weights.\n",
    "        # Setting requires_grad=True indicates that we want to compute gradients with\n",
    "        # respect to these Tensors during the backward pass.\n",
    "#         self.w1 = torch.randn(D_in, H1, device=device, dtype=dtype, requires_grad=True)\n",
    "#         self.w2 = torch.randn(H1,   H2, device=device, dtype=dtype, requires_grad=True)\n",
    "#         self.w3 = torch.randn(H2, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "        return net\n",
    "\n",
    "#     def random_argmax(self, tens):\n",
    "#         \"\"\"Like np.argmax(), but if there are several \"best\" actions,\n",
    "#            chooses and returns one randomly.\n",
    "#            Works with tensors\"\"\"\n",
    "#         arguments = np.argwhere(tens == torch.max(tens)[0]).ravel()\n",
    "#         return np.random.choice(arguments)\n",
    "    \n",
    "    # Lo que me conviene hacer aca es:\n",
    "    # 1. cuando cargo el mapa, extraigo sus estadisticas (puedo repetir esto a la mitad del juego)\n",
    "    # 2. creo diccionarios de int to ascii y ascii to int\n",
    "    # 3. lo unico que llamo desde cada update es es ascii_to_int[c] para cada caracter del mapa\n",
    "    # 4. trabajo directamente con ints o normalizo a algo entre 0 y 1 para que no explote\n",
    "    # From layout of chars to layout of numbers\n",
    "    # Beware: This will be painful to see\n",
    "    def ascii_to_numeric_state_RELOADED(self, ascii_state):\n",
    "        str_state = str(ascii_state)\n",
    "        score_pos = str(str_state).find(\"Score: \")\n",
    "        ascii_map = str(str_state)[:score_pos-1]\n",
    "        \n",
    "        if not hasattr(self, \"sorted_vocab\"):\n",
    "            # first step of all training\n",
    "            #           walls are 0 :O\n",
    "            self.sorted_vocab = ['%','.',' ','\\n','o','G','<','>','^','v']\n",
    "            self.int_to_ascii = {k: w for k, w in enumerate(self.sorted_vocab)}\n",
    "            self.ascii_to_int = {w: k for k, w in self.int_to_ascii.items()}\n",
    "        #print(str_state)\n",
    "        numer_map = [self.ascii_to_int[c] for c in ascii_map]\n",
    "\n",
    "        #numer_map /= 15.0\n",
    "        #last array position will contain the score\n",
    "        #numer_map[-1] = float(str_state[score_pos+7:])/3000\n",
    "        return numer_map\n",
    "    \n",
    "    \n",
    "    def getQValue(self, state, action, compute_grad=False):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        #numer_state = ascii_state_to_numeric_state(state)\n",
    "        #numer_state = ascii_state_to_one_hots_state(state)\n",
    "        numer_state = self.ascii_to_numeric_state_RELOADED(state)\n",
    "        \n",
    "        actions = {'North': [1,0,0,0,0],\n",
    "                   'South': [0,1,0,0,0],\n",
    "                   'East' : [0,0,1,0,0],\n",
    "                   'West' : [0,0,0,1,0],\n",
    "                   'Stop' : [0,0,0,0,1]}\n",
    "        numer_action = actions[action] \n",
    "        input_data = torch.Tensor(np.concatenate((numer_state, numer_action)))#.type(torch.cuda.DoubleTensor)\n",
    "        #print(\"data\",input_data, type(input_data))\n",
    "        if not compute_grad:\n",
    "            # Do not compute grad\n",
    "            with torch.no_grad():\n",
    "                #print(\"not leaving trace. action: \", action)\n",
    "                #out_q = self.net(input_data)\n",
    "                out_q = self.net(torch.Tensor(input_data.repeat(1, 4).view(-1, 60)).reshape((1, 1, 4, len(input_data))))\n",
    "        else:\n",
    "            # Leave trace for calculate grad later\n",
    "            #print(\"leaving trace. action: \", action)\n",
    "            #out_q = self.net(input_data)\n",
    "            out_q = self.net(torch.Tensor(input_data.repeat(1, 4).view(-1, 60)).reshape((1, 1, 4, len(input_data))))\n",
    "        #print(out_q)\n",
    "        return out_q\n",
    "\n",
    "    \n",
    "    def computeActionFromNN(self, state):\n",
    "        \"\"\"\n",
    "          Compute the best action to take in a state.\n",
    "          Returns None if no legal actions\n",
    "        \"\"\"\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            return None\n",
    "\n",
    "        all_q_s_values = np.ndarray(len(legalActions))\n",
    "        for i, a in enumerate(legalActions):\n",
    "            all_q_s_values[i] = self.getQValue(state, a, compute_grad=False)\n",
    "            \n",
    "        #print(legalActions)\n",
    "        #print(\"all_q_s_values\", all_q_s_values)\n",
    "        \n",
    "        best_action = random_argmax(all_q_s_values)\n",
    "        action = legalActions[best_action]\n",
    "        #print(\"action returned\", str(action), type(str(action)))\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def getAction(self, state):\n",
    "        \"\"\"\n",
    "          eps-greedy policy.\n",
    "          Note that if there are no legal actions,\n",
    "          which is the case at the terminal state, you\n",
    "          should choose None as the action.\n",
    "        \"\"\"\n",
    "        # Pick Action\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            return None\n",
    "        #action = None\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # epsilon decay\n",
    "        epsmin = 0.01\n",
    "        eps_decay = 0.9999\n",
    "        #self.epsilon = max(self.epsilon*eps_decay, epsmin)\n",
    "        self.epsilon = 0.1\n",
    "        #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< check\n",
    "        self.epsilon=0.1\n",
    "        if util.flipCoin(self.epsilon):\n",
    "            # Act randomly\n",
    "            action = random.choice(legalActions)\n",
    "        else:\n",
    "            # Act greedly\n",
    "            #action = self.computeActionFromNN(state)\n",
    "            action = self.computeActionFromNN(state)\n",
    "        #print(\"segunda que devuelve action:\", action, type(action))\n",
    "        # Leave trace for calculating grad on update\n",
    "        #_ = self.getQValue(state, action, compute_grad=True)\n",
    "        self.doAction(state, action)\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def getPolQValue(self, state):\n",
    "        \"\"\"\n",
    "          Returns max_action Q(state,action)\n",
    "          where the max is over legal actions.  Note that if\n",
    "          there are no legal actions, which is the case at the\n",
    "          terminal state, you should return a value of 0.0.\n",
    "        \"\"\"\n",
    "\n",
    "        # max_a(Q[state, all actions])\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            #print(\"No legal actions, returning 0\")\n",
    "            #value = 0.0\n",
    "            value = Variable(torch.zeros((1, 1)))\n",
    "        else:\n",
    "            # TODO: Find a better way\n",
    "            policy_action = self.getAction(state)\n",
    "            value = self.getQValue(state, policy_action)\n",
    "            #value=max([self.getQValue(state, a) for a in legalActions])\n",
    "        return value\n",
    "    \n",
    "    \n",
    "    def update(self, state, action, nextState, reward, terminal_state=False):\n",
    "        \"\"\"\n",
    "           Should update your weights based on transition\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        iteration = self.episodesSoFar\n",
    "        self.alpha = max(0.0001, 1/(iteration+1))# los rewards son de orden grande1/(1000*(iteration+1)) # alpha decay\n",
    "        alpha = self.alpha\n",
    "        gamma = 0.9#self.discount\n",
    "\n",
    "\n",
    "        #pastVal = self.getQValue(state, action, compute_grad=True)\n",
    "#         with torch.no_grad():\n",
    "#             advantage = reward + gamma*self.getPolQValue(nextState) - pastVal\n",
    "#         pastVal.backward(-advantage)#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "#         with torch.no_grad():\n",
    "#             crit = nn.MSELoss()\n",
    "        import torch.optim as optim\n",
    "        # create your optimizer\n",
    "        learning_rate = 1e-8\n",
    "        #optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.SGD(self.net.parameters(), lr=learning_rate)\n",
    "        # in your training loop:\n",
    "        for t in range(1):\n",
    "            optimizer.zero_grad()   # zero the gradient buffers\n",
    "            #output = net(input)\n",
    "            #print(\"action grad: \",action)\n",
    "            output = self.getQValue(state, action, compute_grad=True)\n",
    "            if not terminal_state:\n",
    "                target = reward + gamma*self.getPolQValue(nextState)\n",
    "            else:\n",
    "                #print(\"terminal. reward: \", reward)\n",
    "                #target = torch.from_numpy(np.array(reward))\n",
    "                target = reward + gamma*self.getPolQValue(nextState)\n",
    "            #print(target.size())\n",
    "            #print(output.size())\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            #print(\"loss:\", t, loss.item())\n",
    "            optimizer.step()    # Does the update\n",
    "#         if not terminal_state:\n",
    "# #             loss = crit(pastVal, Variable(reward + gamma * self.getPolQValue(nextState)))\n",
    "# #             loss.backward()\n",
    "#             pastVal.backward(reward + gamma*self.getPolQValue(nextState) - pastVal)\n",
    "#             print(reward, action, gamma * self.getPolQValue(nextState), pastVal)\n",
    "#         else:\n",
    "#             # S' is terminal state, we don't want to use q_s'\n",
    "#             pastVal.backward(reward - pastVal)\n",
    "#             print(reward, action, pastVal)\n",
    "#         print()\n",
    "#        with torch.no_grad():\n",
    "#             #print(\"Layer 1:\")\n",
    "#             #print(self.net.fc1.weight.data)\n",
    "#             #print(\"update:\")\n",
    "#             #print(alpha * advantage * self.net.fc1.weight.grad)\n",
    "#                 self.net.fc1.weight.data += alpha * advantage * self.net.fc1.weight.grad\n",
    "#                 #print(\"Layer 1 - Updated\")\n",
    "#                 #print(self.net.fc1.weight.data)\n",
    "#                 self.net.fc2.weight.data += alpha * advantage * self.net.fc2.weight.grad\n",
    "#                 self.net.fc3.weight.data += alpha * advantage * self.net.fc3.weight.grad\n",
    "#                 self.net.fc1.bias.data   += alpha * advantage * self.net.fc1.bias.grad\n",
    "#                 self.net.fc2.bias.data   += alpha * advantage * self.net.fc2.bias.grad\n",
    "#                 self.net.fc3.bias.data   += alpha * advantage * self.net.fc3.bias.grad\n",
    "#            self.net.zero_grad()\n",
    "\n",
    "\n",
    "    def final(self, state):\n",
    "        \"Called at the end of each game.\"\n",
    "        # call the super-class final method\n",
    "        PacmanQAgent.final(self, state)\n",
    "        # did we finish training?\n",
    "        if self.episodesSoFar == self.numTraining:\n",
    "            # you might want to print your weights here for debugging\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class NNQAgentAllQActionsOut(PacmanQAgent):\n",
    "    \"\"\"\n",
    "       ApproximateQLearningAgent\n",
    "\n",
    "       You should only have to overwrite getQValue\n",
    "       and update.  All other QLearningAgent functions\n",
    "       should work as is.\n",
    "    \"\"\"\n",
    "    def __init__(self, extractor='IdentityExtractor', **args):\n",
    "        #extractor = 'CoordinateExtractor'\n",
    "        #self.featExtractor = util.lookup(extractor, globals())()\n",
    "        PacmanQAgent.__init__(self, **args)\n",
    "        #self.weights = util.Counter()\n",
    "        self.net = self.initNN()\n",
    "        #self.net = self.net.to('cuda:0')\n",
    "        # to float; test with double later\n",
    "        #self.net = self.net.float()\n",
    "        del self.Q\n",
    "        \n",
    "        \n",
    "    def initNN(self):\n",
    "        net = Net()\n",
    "#         torch.nn.init.xavier_uniform(net.fc1.weight.data)\n",
    "#         torch.nn.init.xavier_uniform(net.fc2.weight.data)\n",
    "#         torch.nn.init.xavier_uniform(net.fc3.weight.data)\n",
    "        torch.nn.init.uniform_(net.fc1.weight.data, 0.0, 0.001)\n",
    "        torch.nn.init.uniform_(net.fc2.weight.data, 0.0, 0.001)\n",
    "        torch.nn.init.uniform_(net.fc3.weight.data, 0.0, 0.001)\n",
    "        torch.nn.init.uniform_(net.fc1.bias.data,   0.0, 0.001)\n",
    "        torch.nn.init.uniform_(net.fc2.bias.data,   0.0, 0.001)\n",
    "        torch.nn.init.uniform_(net.fc3.bias.data,   0.0, 0.001)\n",
    "        #torch.nn.init.xavier_uniform(net.weight)\n",
    "        #net.bias.data.fill_(0.01)\n",
    "        # Create random Tensors for weights.\n",
    "        # Setting requires_grad=True indicates that we want to compute gradients with\n",
    "        # respect to these Tensors during the backward pass.\n",
    "#         self.w1 = torch.randn(D_in, H1, device=device, dtype=dtype, requires_grad=True)\n",
    "#         self.w2 = torch.randn(H1,   H2, device=device, dtype=dtype, requires_grad=True)\n",
    "#         self.w3 = torch.randn(H2, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "        return net\n",
    "\n",
    "#     def random_argmax(self, tens):\n",
    "#         \"\"\"Like np.argmax(), but if there are several \"best\" actions,\n",
    "#            chooses and returns one randomly.\n",
    "#            Works with tensors\"\"\"\n",
    "#         arguments = np.argwhere(tens == torch.max(tens)[0]).ravel()\n",
    "#         return np.random.choice(arguments)\n",
    "    \n",
    "    # Lo que me conviene hacer aca es:\n",
    "    # 1. cuando cargo el mapa, extraigo sus estadisticas (puedo repetir esto a la mitad del juego)\n",
    "    # 2. creo diccionarios de int to ascii y ascii to int\n",
    "    # 3. lo unico que llamo desde cada update es es ascii_to_int[c] para cada caracter del mapa\n",
    "    # 4. trabajo directamente con ints o normalizo a algo entre 0 y 1 para que no explote\n",
    "    # From layout of chars to layout of numbers\n",
    "    # Beware: This will be painful to see\n",
    "    def ascii_to_numeric_state_RELOADED(self, ascii_state):\n",
    "        str_state = str(ascii_state)\n",
    "        score_pos = str(str_state).find(\"Score: \")\n",
    "        ascii_map = str(str_state)[:score_pos-1]\n",
    "        \n",
    "        if not hasattr(self, \"sorted_vocab\"):\n",
    "            # first step of all training\n",
    "            #           walls are 0 :O\n",
    "            self.sorted_vocab = ['%','.',' ','\\n','o','G','<','>','^','v']\n",
    "            self.int_to_ascii = {k: w for k, w in enumerate(self.sorted_vocab)}\n",
    "            self.ascii_to_int = {w: k for k, w in self.int_to_ascii.items()}\n",
    "        #print(str_state)\n",
    "        numer_map = torch.Tensor([self.ascii_to_int[c] for c in ascii_map])\n",
    "        \n",
    "\n",
    "        #numer_map /= 15.0\n",
    "        #last array position will contain the score\n",
    "        #numer_map[-1] = float(str_state[score_pos+7:])/3000\n",
    "        return numer_map.view(-1, len(str_state)).reshape(1, len(self.sorted_vocab), len(str_state))\n",
    "    \n",
    "    \n",
    "    def getQValues(self, state, compute_grad=False):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        #numer_state = ascii_state_to_numeric_state(state)\n",
    "        #numer_state = ascii_state_to_one_hots_state(state)\n",
    "        #numer_state = self.ascii_to_numeric_state_RELOADED(state)\n",
    "        input_data =  self.ascii_to_numeric_state_RELOADED(state)\n",
    "#         actions = {'North': [1,0,0,0,0],\n",
    "#                    'South': [0,1,0,0,0],\n",
    "#                    'East' : [0,0,1,0,0],\n",
    "#                    'West' : [0,0,0,1,0],\n",
    "#                    'Stop' : [0,0,0,0,1]}\n",
    "#         numer_action = actions[action] \n",
    "#         input_data = torch.Tensor(np.concatenate((numer_state, numer_action)))#.type(torch.cuda.DoubleTensor)\n",
    "        #print(\"data\",input_data, type(input_data))\n",
    "        if not compute_grad:\n",
    "            # Do not compute grad\n",
    "            with torch.no_grad():\n",
    "                #print(\"not leaving trace. action: \", action)\n",
    "                #out_q = self.net(input_data)\n",
    "                #out_q = self.net(torch.Tensor(input_data.repeat(1, 4).view(-1, 60)).reshape((1, 1, 4, len(input_data))))\n",
    "                out_q = self.net(input_data.reshape((1, 1, len(self.sorted_vocab), len(input_data))))\n",
    "        else:\n",
    "            # Leave trace for calculate grad later\n",
    "            #print(\"leaving trace. action: \", action)\n",
    "            #out_q = self.net(input_data)\n",
    "            out_q = self.net(torch.Tensor(input_data.repeat(1, 4).view(-1, 60)).reshape((1, 1, 4, len(input_data))))\n",
    "        #print(out_q)\n",
    "        return out_q\n",
    "\n",
    "    \n",
    "    def computeActionFromNN(self, state):\n",
    "        \"\"\"\n",
    "          Compute the best action to take in a state.\n",
    "          Returns None if no legal actions\n",
    "        \"\"\"\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            return None\n",
    "\n",
    "#         all_q_s_values = np.ndarray(len(legalActions))\n",
    "#         for i, a in enumerate(legalActions):\n",
    "#             all_q_s_values[i] = self.getQValue(state, a, compute_grad=False)\n",
    "        all_q_s_values = self.getQValues(state, compute_grad=False)\n",
    "        #print(legalActions)\n",
    "        #print(\"all_q_s_values\", all_q_s_values)\n",
    "        print(all_q_s_values)\n",
    "        best_action = random_argmax(all_q_s_values)\n",
    "        action = legalActions[best_action]\n",
    "        #print(\"action returned\", str(action), type(str(action)))\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def getAction(self, state):\n",
    "        \"\"\"\n",
    "          eps-greedy policy.\n",
    "          Note that if there are no legal actions,\n",
    "          which is the case at the terminal state, you\n",
    "          should choose None as the action.\n",
    "        \"\"\"\n",
    "        # Pick Action\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            return None\n",
    "        #action = None\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # epsilon decay\n",
    "        epsmin = 0.01\n",
    "        eps_decay = 0.9999\n",
    "        #self.epsilon = max(self.epsilon*eps_decay, epsmin)\n",
    "        self.epsilon = 0.1\n",
    "        #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< check\n",
    "        self.epsilon=0.1\n",
    "        if util.flipCoin(self.epsilon):\n",
    "            # Act randomly\n",
    "            action = random.choice(legalActions)\n",
    "        else:\n",
    "            # Act greedly\n",
    "            #action = self.computeActionFromNN(state)\n",
    "            action = self.computeActionFromNN(state)\n",
    "        #print(\"segunda que devuelve action:\", action, type(action))\n",
    "        # Leave trace for calculating grad on update\n",
    "        #_ = self.getQValue(state, action, compute_grad=True)\n",
    "        self.doAction(state, action)\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def getPolQValue(self, state):\n",
    "        \"\"\"\n",
    "          Returns max_action Q(state,action)\n",
    "          where the max is over legal actions.  Note that if\n",
    "          there are no legal actions, which is the case at the\n",
    "          terminal state, you should return a value of 0.0.\n",
    "        \"\"\"\n",
    "\n",
    "        # max_a(Q[state, all actions])\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            #print(\"No legal actions, returning 0\")\n",
    "            #value = 0.0\n",
    "            value = Variable(torch.zeros((1, 1)))\n",
    "        else:\n",
    "            # TODO: Find a better way\n",
    "            policy_action = self.getAction(state)\n",
    "            value = self.getQValue(state, policy_action)\n",
    "            #value=max([self.getQValue(state, a) for a in legalActions])\n",
    "        return value\n",
    "    \n",
    "    \n",
    "    def update(self, state, action, nextState, reward, terminal_state=False):\n",
    "        \"\"\"\n",
    "           Should update your weights based on transition\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        iteration = self.episodesSoFar\n",
    "        self.alpha = max(0.0001, 1/(iteration+1))# los rewards son de orden grande1/(1000*(iteration+1)) # alpha decay\n",
    "        alpha = self.alpha\n",
    "        gamma = 0.9#self.discount\n",
    "\n",
    "\n",
    "        #pastVal = self.getQValue(state, action, compute_grad=True)\n",
    "#         with torch.no_grad():\n",
    "#             advantage = reward + gamma*self.getPolQValue(nextState) - pastVal\n",
    "#         pastVal.backward(-advantage)#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "#         with torch.no_grad():\n",
    "#             crit = nn.MSELoss()\n",
    "        import torch.optim as optim\n",
    "        # create your optimizer\n",
    "        learning_rate = 1e-8\n",
    "        #optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.SGD(self.net.parameters(), lr=learning_rate)\n",
    "        # in your training loop:\n",
    "        for t in range(1):\n",
    "            optimizer.zero_grad()   # zero the gradient buffers\n",
    "            #output = net(input)\n",
    "            #print(\"action grad: \",action)\n",
    "            #output = self.getQValues(state, compute_grad=True)\n",
    "            if not terminal_state:\n",
    "                target = reward + gamma*self.getPolQValue(nextState)\n",
    "            else:\n",
    "                #print(\"terminal. reward: \", reward)\n",
    "                #target = torch.from_numpy(np.array(reward))\n",
    "                target = reward + gamma*self.getPolQValue(nextState)\n",
    "            #print(target.size())\n",
    "            #print(output.size())\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            #print(\"loss:\", t, loss.item())\n",
    "            optimizer.step()    # Does the update\n",
    "#         if not terminal_state:\n",
    "# #             loss = crit(pastVal, Variable(reward + gamma * self.getPolQValue(nextState)))\n",
    "# #             loss.backward()\n",
    "#             pastVal.backward(reward + gamma*self.getPolQValue(nextState) - pastVal)\n",
    "#             print(reward, action, gamma * self.getPolQValue(nextState), pastVal)\n",
    "#         else:\n",
    "#             # S' is terminal state, we don't want to use q_s'\n",
    "#             pastVal.backward(reward - pastVal)\n",
    "#             print(reward, action, pastVal)\n",
    "#         print()\n",
    "#        with torch.no_grad():\n",
    "#             #print(\"Layer 1:\")\n",
    "#             #print(self.net.fc1.weight.data)\n",
    "#             #print(\"update:\")\n",
    "#             #print(alpha * advantage * self.net.fc1.weight.grad)\n",
    "#                 self.net.fc1.weight.data += alpha * advantage * self.net.fc1.weight.grad\n",
    "#                 #print(\"Layer 1 - Updated\")\n",
    "#                 #print(self.net.fc1.weight.data)\n",
    "#                 self.net.fc2.weight.data += alpha * advantage * self.net.fc2.weight.grad\n",
    "#                 self.net.fc3.weight.data += alpha * advantage * self.net.fc3.weight.grad\n",
    "#                 self.net.fc1.bias.data   += alpha * advantage * self.net.fc1.bias.grad\n",
    "#                 self.net.fc2.bias.data   += alpha * advantage * self.net.fc2.bias.grad\n",
    "#                 self.net.fc3.bias.data   += alpha * advantage * self.net.fc3.bias.grad\n",
    "#            self.net.zero_grad()\n",
    "\n",
    "\n",
    "    def final(self, state):\n",
    "        \"Called at the end of each game.\"\n",
    "        # call the super-class final method\n",
    "        PacmanQAgent.final(self, state)\n",
    "        # did we finish training?\n",
    "        if self.episodesSoFar == self.numTraining:\n",
    "            # you might want to print your weights here for debugging\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: usar los 3 canales de colores de entrada para dividir los tipos\n",
    "#       de cada simbolo en un one-hot vector (con valor) de dimension 3,\n",
    "#       pudiendo representar: inerte (paredes/espacio vacio),\n",
    "#                             activo (pellets/items),\n",
    "#                             agentes (pacman, ghosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Shameless copied from:\n",
    "@author: Viet Nguyen <nhviet1009@gmail.com>/github:vietnguyen91\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GlobalAdam(torch.optim.Adam):\n",
    "    def __init__(self, params, lr):\n",
    "        super(GlobalAdam, self).__init__(params, lr=lr)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = 0\n",
    "                state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                state['exp_avg'].share_memory_()\n",
    "                state['exp_avg_sq'].share_memory_()\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.lstm = nn.LSTMCell(32 * 6 * 6, 512)\n",
    "        self.critic_linear = nn.Linear(512, 1)\n",
    "        self.actor_linear = nn.Linear(512, num_actions)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                # nn.init.kaiming_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.LSTMCell):\n",
    "                nn.init.constant_(module.bias_ih, 0)\n",
    "                nn.init.constant_(module.bias_hh, 0)\n",
    "\n",
    "    def forward(self, x, hx, cx):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        hx, cx = self.lstm(x.view(x.size(0), -1), (hx, cx))\n",
    "        return self.actor_linear(hx), self.critic_linear(hx), hx, cx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElAgenteMasDificilDeLaHistoria(PacmanQAgent):\n",
    "    \"Aqui se supone que implemento actor-critic _/¯(ツ)¯\\_\"\n",
    "    def __init__(self, epsilon=0.05, gamma=0.8, alpha=0.2, numTraining=0, **args):\n",
    "        self.index = 0  # This is always Pacman\n",
    "        QLearningAgent.__init__(self, **args)\n",
    "        self.Q = None\n",
    "        \n",
    "    def local_train(index, opt, global_model, optimizer, save=False):\n",
    "        torch.manual_seed(123 + index)\n",
    "        if save:\n",
    "            start_time = timeit.default_timer()\n",
    "        writer = SummaryWriter(opt.log_path)\n",
    "        env, num_states, num_actions = create_train_env(opt.world, opt.stage, opt.action_type)\n",
    "        local_model = ActorCritic(num_states, num_actions)\n",
    "        if opt.use_gpu:\n",
    "            local_model.cuda()\n",
    "        local_model.train()\n",
    "        state = torch.from_numpy(env.reset())\n",
    "        if opt.use_gpu:\n",
    "            state = state.cuda()\n",
    "        done = True\n",
    "        curr_step = 0\n",
    "        curr_episode = 0\n",
    "        while True:\n",
    "#             if save:\n",
    "#                 if curr_episode % opt.save_interval == 0 and curr_episode > 0:\n",
    "#                     torch.save(global_model.state_dict(),\n",
    "#                                \"{}/a3c_super_mario_bros_{}_{}\".format(opt.saved_path, opt.world, opt.stage))\n",
    "#                 print(\"Process {}. Episode {}\".format(index, curr_episode))\n",
    "            curr_episode += 1\n",
    "            local_model.load_state_dict(global_model.state_dict())\n",
    "            if done:\n",
    "                # zero gradients\n",
    "                h_0 = torch.zeros((1, 512), dtype=torch.float)\n",
    "                c_0 = torch.zeros((1, 512), dtype=torch.float)\n",
    "            else:\n",
    "                h_0 = h_0.detach() # no grad\n",
    "                c_0 = c_0.detach() # no grad\n",
    "            if opt.use_gpu:\n",
    "                h_0 = h_0.cuda()\n",
    "                c_0 = c_0.cuda()\n",
    "\n",
    "            log_policies = []\n",
    "            values = []\n",
    "            rewards = []\n",
    "            entropies = []\n",
    "\n",
    "            for _ in range(opt.num_local_steps):\n",
    "                curr_step += 1\n",
    "                logits, value, h_0, c_0 = local_model(state, h_0, c_0)\n",
    "                policy = F.softmax(logits, dim=1)\n",
    "                log_policy = F.log_softmax(logits, dim=1)\n",
    "                entropy = -(policy * log_policy).sum(1, keepdim=True)\n",
    "\n",
    "                m = Categorical(policy)\n",
    "                action = m.sample().item()\n",
    "\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                state = torch.from_numpy(state)\n",
    "                if opt.use_gpu:\n",
    "                    state = state.cuda()\n",
    "                if curr_step > opt.num_global_steps:\n",
    "                    done = True\n",
    "\n",
    "                if done:\n",
    "                    curr_step = 0\n",
    "                    state = torch.from_numpy(env.reset())\n",
    "                    if opt.use_gpu:\n",
    "                        state = state.cuda()\n",
    "\n",
    "                values.append(value)\n",
    "                log_policies.append(log_policy[0, action])\n",
    "                rewards.append(reward)\n",
    "                entropies.append(entropy)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            R = torch.zeros((1, 1), dtype=torch.float)\n",
    "            if opt.use_gpu:\n",
    "                R = R.cuda()\n",
    "            if not done:\n",
    "                _, R, _, _ = local_model(state, h_0, c_0)\n",
    "\n",
    "            gae = torch.zeros((1, 1), dtype=torch.float)\n",
    "            if opt.use_gpu:\n",
    "                gae = gae.cuda()\n",
    "            actor_loss = 0\n",
    "            critic_loss = 0\n",
    "            entropy_loss = 0\n",
    "            next_value = R\n",
    "\n",
    "            for value, log_policy, reward, entropy in list(zip(values, log_policies, rewards, entropies))[::-1]:\n",
    "                gae = gae * opt.gamma * opt.tau\n",
    "                gae = gae + reward + opt.gamma * next_value.detach() - value.detach()\n",
    "                next_value = value\n",
    "                actor_loss = actor_loss + log_policy * gae\n",
    "                R = R * opt.gamma + reward\n",
    "                critic_loss = critic_loss + (R - value) ** 2 / 2\n",
    "                entropy_loss = entropy_loss + entropy\n",
    "\n",
    "            total_loss = -actor_loss + critic_loss - opt.beta * entropy_loss\n",
    "            writer.add_scalar(\"Train_{}/Loss\".format(index), total_loss, curr_episode)\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "\n",
    "            for local_param, global_param in zip(local_model.parameters(), global_model.parameters()):\n",
    "                if global_param.grad is not None:\n",
    "                    break\n",
    "                global_param._grad = local_param.grad\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if curr_episode == int(opt.num_global_steps / opt.num_local_steps):\n",
    "                print(\"Training process {} terminated\".format(index))\n",
    "                if save:\n",
    "                    end_time = timeit.default_timer()\n",
    "                    print('The code runs for %.2f s ' % (end_time - start_time))\n",
    "                return    \n",
    "        \n",
    "        \n",
    "        \n",
    "    def update(self, state, action, nextState, reward, terminal_state=False):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def getAction(self, state):\n",
    "        \"\"\"\n",
    "        Simply calls the getAction method of QLearningAgent and then\n",
    "        informs parent of action for Pacman.  Do not change or remove this\n",
    "        method.\n",
    "        \"\"\"\n",
    "        # Pick Action\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            return None\n",
    "        \n",
    "        action = random.choice(legalActions)\n",
    "        self.doAction(state, action)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T02:08:52.906228Z",
     "start_time": "2019-06-22T02:08:52.893918Z"
    }
   },
   "outputs": [],
   "source": [
    "# from subprocess import call\n",
    "# command = ('ipython nbconvert --to script qlearningAgents.ipynb')\n",
    "# call(command, shell=True)\n",
    "\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T15:37:25.404889Z",
     "start_time": "2019-06-21T15:37:25.374012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([np.array(1),2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T13:32:34.269845Z",
     "start_time": "2019-06-21T13:32:34.203680Z"
    }
   },
   "source": [
    "Small Classic Layout (7x20):\n",
    "\n",
    "    %%%%%%%%%%%%%%%%%%%%\n",
    "    %......%G  G%......%\n",
    "    %.%%...%%  %%...%%.%\n",
    "    %.%o.%........%.o%.%\n",
    "    %.%%.%.%%%%%%.%.%%.%\n",
    "    %........P.........%\n",
    "    %%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "I need it as input of my NN, flatting it out (1x140):\n",
    "\n",
    "    %%%%%%%%%%%%%%%%%%%%%......%G  G%......%%.%%...%%  %%...%%.%%.%o.%........%.o%.%%.%%.%.%%%%%%.%.%%.%%........P.........%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "Still need numbers/one-hot vectors for each symbol? or will it work without?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T19:15:03.483366Z",
     "start_time": "2019-06-16T19:15:03.469464Z"
    }
   },
   "outputs": [],
   "source": [
    "# state:  \n",
    "# %%%%%%%\n",
    "# %  G> %\n",
    "# % %%% %\n",
    "# % %.  %\n",
    "# % %%% %\n",
    "# %.    %\n",
    "# %%%%%%%\n",
    "# Score: -8\n",
    "\n",
    "# action:  East"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
